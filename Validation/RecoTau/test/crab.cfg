[CRAB]
 
jobtype = cmssw
scheduler =glite
#scheduler = glidein
#server_name = caf
use_server = 1
###### per runnare su RelVal #####   https://twiki.cern.ch/twiki/bin/view/CMS/RelValHarvesting
#As the datasets are hosted are CERN and FNAL, the following crab settings are needed for accessing the data
#-remove the automatic T1 blacklist, with remove_default_blacklist
#-enable crab to show data hosted on Tier1s sites specify, with show_prod , as default those data are masked
#[CMSSW]
#show_prod = 1
#
#[EDG]
#remove_default_blacklist=1
#rb = CERN
#This is done in the automatically generated configs
###################
[CMSSW]
### The data you want to access (to be found on DBS) 
pycfg_params = noprint
datasetpath=/DYToTauTau_M-20_TuneZ2_7TeV-pythia6-tauola/Spring11-E7TeV_FlatDist10_2011EarlyData_50ns_START311_V1G1-v1/AODSIM

### The ParameterSet you want
#pset=SkimReco_BsToJpsif.py
pset=RunValidation_cfg.py
pycfg_params = gridJob=True conditions=START311_V1G1::All
#pset=test_flags.py
####jsonfile
###Only for Data


#lumi_mask=/nfs/data6/bmillanm/BsJpsiPhi/CMSSW_3_8_6/src/HeavyFlavorAnalysis/Onia2MuMu/certification/7TeV/Collisions10/Reprocessing/Cert_136033-149442_7TeV_Nov4ReReco_Collisions10_JSON_BPAG.txt
#lumi_mask=/nfs/data6/bmillanm/BsJpsiPhi/CMSSW_3_8_4_patch3/src/HeavyFlavorAnalysis/Onia2MuMu/certification/7TeV/Collisions10/StreamExpress/Cert_132440-149442_7TeV_StreamExpress_Collisions10_JSON_BPAG_v2.txt
#lumi_mask=/nfs/data6/bmillanm/BsJpsiPhi/CMSSW_3_9_7/src/HeavyFlavorAnalysis/Onia2MuMu/certification/7TeV/Collisions10/Reprocessing/146000_147210.txt
#lumi_mask=/nfs/data6/bmillanm/BsJpsiPhi/CMSSW_3_9_7/src/HeavyFlavorAnalysis/Onia2MuMu/certification/7TeV/Collisions10/Reprocessing/147219_149294.txt
#lumi_mask=/nfs/data6/bmillanm/BsJpsiPhi/CMSSW_3_6_1_patch4/src/HeavyFlavorAnalysis/BsToJpsiPhi/Json/json_PD10B_148059_149181.txt

### Total number of events to be accessed: -1 means all ("-1" is not usable if no input)
#total_number_of_events=-1
#### In data Lumis per job and number of jobs.
### Analogous to events, lumis are used to split up analysis dataset
#total_number_of_lumis=-1
#lumis_per_job = 1
### Number of jobs
# for Data
##number of jobs is enough
number_of_jobs=5
total_number_of_events=-1




### The output files produced by your application (comma separated list)

#output_file=20101109_PD10A_140042_144114.root
output_file=TauVal_CMSSW_3_11_1_patch2_ZTT.root
###output_file=2011201_test_BsJpsif_ReReco_CMSSW386.root



#CE_black_list=hephyse.oeaw.ac.at
[USER]

## to have back the job executable output into UI (return_data= 1)
return_data = 0

### COPY JOB OUTPUT INTO ###
copy_data = 1
email=mverzett@cern.ch

### if copy_data = 1 ###
storage_element=srm-cms.cern.ch
storage_path=/srm/managerv2?SFN=/castor/cern.ch
user_remote_dir=/user/m/mverzett/store/


publish_data=0
## IMPORTANT create the dir in castor (e.g.)
##           add the permission to it or all the jobs will crash :-)
#rfmkdir /castor/cern.ch/user/u/username/subdir 
#rfchmod +775 /castor/cern.ch/user/u/username/subdir

