%         FewKDE Algorithm
%-------------------------------------------------------------------
\documentclass[prd,twocolumn]{revtex4}
\newif\ifpdf \ifx\pdfoutput\undefined \pdffalse \else \pdftrue \fi 
\ifpdf \usepackage[pdftex]{graphicx} \else \usepackage{graphicx} \fi 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{journals}
\usepackage{color}
\ifpdf
 \usepackage[colorlinks,hyperindex]{hyperref}
\else
 \newcommand{\href}[2]{#2}                   % suppress URL's in PS files
 \usepackage[dvips,pagebackref]{hyperref}    % create page-back-refrences in ps files
\fi 

\bibliographystyle{apsrev}


\def\FewKDE{{\sc FewKDE}}
\def\nMC{N}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfigure}
\newif\ifpdf \ifx\pdfoutput\undefined \pdffalse \else \pdftrue \fi 
\ifpdf \usepackage[pdftex]{graphics} \else \usepackage{graphics} \fi 

\newcommand {\abs}[1]{\left| #1 \right|}
\newcommand {\floor}[1]{\left\lfloor #1 \right\rfloor}

\renewcommand\topmargin{0in}

\begin{document}

\title{\FewKDE: A Fast Kernel Estimator}
\author{Bruce Knuteson}
\homepage{http://mit.fnal.gov/~knuteson/}
\email{knuteson@mit.edu}
\affiliation{Massachusetts Institute of Technology}
\author{Sheel Dandekar}
\homepage{http://mit.fnal.gov/~sdandek/}
\email{sdandek@mit.edu}
\affiliation{Massachusetts Institute of Technology}
\keywords{multivariate, kernel density estimation}

\date{\today}

\begin{abstract}
The evaluation of the standard non-parametric kernel density estimator at a single point requires the computation of $\nMC$ exponentials, where $\nMC$ is the number of Monte Carlo points defining the sample whose density is to be estimated.  This leads to an algorithm whose run time scales as ${\cal O}(\nMC^2)$ when the density needs to evaluated at each of those points.  For $\nMC \gg 10^4$, a situation commonly encountered in high energy physics, the time required to use the standard kernel density estimator is prohibitive.  This article proposes an alternative non-parametric estimator in which kernels are iteratively placed, their positions and orientations chosen to maximize the probability that the estimate would produce the Monte Carlo events at hand.
\end{abstract}

\maketitle

%===============================================================
\tableofcontents

\section{The problem}

Kernel density estimation provides a method for estimating the parent distribution of a sample of discrete Monte Carlo points.  The point could be single or multidimensional, and represents an event, which could be a number of things.  One possible example is that each point could represent a hurricane, with the various dimensions of the point referring to different attributes of the hurricane, such as peak wind speed, the velocity at which the eye travels, etc.  Since real hurricanes do not occur that frequently in most places, a computer simulation could be designed to simulate a large number of hurricane events.  Such an approach is particularly useful in cases where the actual event, in this case a hurricane, is difficult to describe with a simple set of equations, and is also affected by a large number of variables.  Another case for which Monte Carlo points could be used is the collisions occuring inside a particle accelerator.  These collisions can be represented by 4-vectors, where the dimensions correspond to the energy of the particle, the mass, and two directional variables specifying the angle at which it travels.  Different hypotheses for the interactions between the particles can be tested by generating a large number of Monte Carlo points according to the hypothesis, and then comparing them to the data from the accelerator.

 In this article we consider the general case in which it is desired to estimate the parent distribution of $\nMC$ Monte Carlo points.  To find the parent distribution, a number of approaches are available.  In many instances, it is very desirable that the resulting distribution be smooth.  It is also valuable, especially in certain high-energy physics applications, to have an algorithm that can give parent distributions with hard physical boundaries.  For example, in high-energy physics there are strict barriers on what energies can currently be produced.  There are also sharp cutoffs used to identify a particle as say, an electron.  In this paper, we will consider standard kernel density estimation, adaptive kernel density estimation, and the authors' proposed solution, FewKDE.  

\begin{figure}
\label{fig:VariousEstimators}
%\includegraphics[angle = 270, width=3in]{twobythreefigure}
\caption{\ Different methods of estimating the parent distribution.}
\end{figure}

Each Monte Carlo point $i$ has variable values $\vec{x}_i$ within a $d$-dimensional variable space ${\cal V}$; each Monte Carlo point $i$ also has its own unique weight $w_i$.  We can represent these points as a series of tick marks along an axis, placing a delta function at each point.  In Fig.~\ref{fig:VariousEstimators} (a), this scenario is drawn, with the heights of the delta-functions indicating their weights.  In (b), the delta functions have been replaced by narrow Gaussians centered at each point.  The integrals of each Gaussian equals the weight of the corresponding point.  In (c), a histogram is shown.  This histogram is created by first selecting bins, and then making the height of each bar the density of the distribution.  This is calculated by taking the total number of events occurring in each bin and dividing by the bin width. (d), (e), and (f) show the standard kernel density estimate, adaptive kernel density estimate, and the authors' proposed estimate, FewKDE, respectively.  In each case, the overall kernel estimate (top curve) is the sum of a number of smaller kernels.

What is a kernel estimate?  A kernel estimate comprises one or more kernels, and one weight for each kernel.  The kernel estimate is an estimate for the probability distribution function of the parent distribution, and is the sum of the individual kernels.  The kernel estimate $K$ is a weighted sum of the $n$ individual kernels $K_k$, each with weight $w_k$.  Throughout this article, the index $i$, ranging from 1 to $\nMC$, will be used consistently to index the Monte Carlo points.  The index $k$, ranging from 1 to $n$, will be used to index the kernels.  Hence $w_i$ denotes the weight of the $i^{\text{th}}$ Monte Carlo, and $w_k$ denotes the weight of the $k^{\text{th}}$ kernel.  $K$ can be represented schematically as:
\begin{equation}
K = \sum_{k=1}^{n}{w_k K_k}.
\end{equation}
We require $\sum_k{w_k}=1$, so that the integral of the kernel estimate $K$ over the entire variable space ${\cal V}$ is unity.  For our purposes the kernels will be taken to be Gaussian, since both Gaussians and their sums are smooth.

Two of the most popular approaches to our problem are the standard kernel density estimator and the adaptive kernel density estimator.  In both approaches, a kernel is centered around each point, so the mean of the kernel is given by the vector $\vec{x}_i$.  We will write the standard estimate as $K_{Standard}$ and the adaptive estimate as $K_{Adaptive}$.  For both these estimators, the canonical form is given by:

\begin{equation}
\label{eqn:CanonicalFormofKDE}
K(\vec{x}_i) = \sum_{i=1}^N w_i \frac {1}{\sqrt{2\pi}^d h^d \abs{\sum}} e^-{\frac {(\vec{x} - \vec{x}_i)^T {\sum}^{-1} (\vec{x} - \vec{x}_i)} {2h^d}}
\end{equation}

The weight for each point is normalized, by summing all the weights and dividing each weight by the total weight.  This ensures that the resulting summation over all Monte Carlo points will be properly normalized.   We are considering the case where the variables for each point are not necessarily independent, so the values of two variables can be correlated. As a result, we must consider the covariance matrix, which is a dxd matrix denoted by $\sum$:

\begin{equation}
\label{eqn:CovarianceMatrix}
\sum_{kl} = \frac {1}{N} \sum_{i=0}^N((\vec{x_i})_l - \langle \vec{x} \rangle_l) ((\vec{x_i})_k - \langle \vec{x} \rangle_k) 
\end{equation}

  This covariance matrix is multiplied by a parameter $h$, which is raised to the $d$ power.  The smoothing parameter is the only difference between the standard kernel density estimate and the adaptive kernel density estimate; in the standard kernel density estimate, $h$ is the same for each Monte Carlo point, while it varies in the adaptive kernel density estimate.  The exponent here looks rather messy, but it is not that far from the usual 1-dimensional Gaussian exponent of $\frac{-(x-x_i)^2}{2\sigma_x^2}$.  Here, however, there is a $\frac{{\sum}^{-1}} {h^d}$ term sandwiched in the middle.  If this term is thought of as multidimensional equivalent of $\frac {1}{\sigma^2}$, the exponent becomes more comprehensible.  From Eq.~\ref{eqn:CanonicalFormofKDE}, one can see that each kernel is defined by its $d$-dimensional means and variances. 

The smoothing parameter $h$ is a positive number smaller than 1.  Its purpose is to try to even out bumps that may be formed when the kernels are summed to produce the kernel estimate.  The area underneath each kernel remains unchanged by the smoothing, so as its width becomes smaller, the kernel becomes more sharply peaked.  The smaller the value of $h$, the greater the effect on the corresponding kernel.  For standard kernel density estimation, the smoothing parameter is given by:

\begin{equation}
h^* = N^{-1/(d+4)}
\end{equation}

For adaptive kernel density estimation, the parameter $h_i$ corresponding to the kernel associated with the $i^{\text{th}}$ Monte Carlo point is different for each kernel.  In order to calculate $h_i$, the standard kernel density estimate must first be calculated.

\begin{equation}
h_i = h^* (\frac {K_{simple}(\vec{x_i})} {\frac {1}{N} \sum_{i=1}^N {K_{simple}(\vec{x_i})}})^{-\alpha}
\end{equation}

Here $K_{simple}(\vec{x_i})$ denotes the standard kernel density estimate at the point $\vec{x_i}$.  If the density at a given point is greater than the average mean density, the smoothing parameter will become smaller than in the standard kernel density estimate, which will make the kernel narrower and more sharply peaked.  Conversely, if the density is less than the mean density, the smoothing parameter will become larger, and so the kernel will become shorter and flatter.  For this reason, the exponent $\alpha$ must have a negative sign.  Empirically, a good choice for $\alpha$ is .5 [Silverman].  

Both algorithms have some trouble dealing with distributions with sharp cutoffs.  Because these two estimates yield continuous distributions, whereas the actual parent distribution might only be piecewise continuous, the estimate is unreliable near the ends of the distribution.  In particular, these estimates have trouble finding peaks near the ends of the distribution.  Another problem is that the smoothing procedure rounds out any discontinuous features that may actually be present in the parent distribution.  In the example of the particle accelerator, this is bad because the detector has an abrupt cutoff for what energies it is able to measure.

They also have a cost: the time required to run these algorithms scales as ${\cal O}(N^2)$.  For any calculation such as an expectation value of a particular quantity, the value of the estimate must be calculated at all N points.  Evaluation of at any of these points involves summing the value of N exponentials (one for each kernel).  The calculation of an exponential typically takes a computer on the order of 100 nanoseconds.  So for more than $10^6$ points, these methods are not practical.  Neural networks are a common alternative to these methods, because they run much faster. 

\section{FewKDE}

\FewKDE\ constructs a kernel estimate from a few kernels, rather than using one kernel for each Monte Carlo point.  Evaluation of the estimate at a particular point $\vec{x} \in {\cal V}$ hence requires the evaluation of only a few kernels, rather than $\nMC$ of them, with an enormous savings in time for large $\nMC$.  This estimate will be written as $K_{Few}$.

\subsection{Form}

In order to adeptly handle the hard physical boundaries in commonly occurring distributions, we determine the smallest ($d$-dimensional) rectangle, with sides parallel to the axes of each of the $d$ variables, that completely encompasses all $\nMC$ Monte Carlo points.  This rectangle defines a finite area of support for the resulting density; the density estimate is set to zero outside this rectangle, and each kernel is normalized so that its integral within the bounding rectangle is unity.  In this algorithm, the kernels are not centered at data points; in fact, it is not even necessary that the mean of the kernel fall within the bounding rectangle. [Sec. ~\ref{sec:Examples}]. 

Each kernel is therefore described by $2d$ parameters, which specify the mean and variance of the $d$ univariate product Gaussians, and $2d$ numbers, common among all kernels, which specify the high and low ends of the bounding rectangle.  The total number of parameters characterizing the estimate is thus 
\begin{equation}
n-1 + 2 \, d \, n + 2 \, d,
\end{equation}
accounting for the $n-1$ kernel weights, the $2d$ parameters for each of $n$ kernels, and the $2d$ numbers specifying the area of support.  It is sufficient for our purposes to note that the number of parameters scales as ${\cal O}(n \, d)$.

\subsection{Construction}

The space of all possible kernel approximations is very large, due to the large number of parameters in the above equation that can be adjusted.  We therefore need a good method for determining how well a particular kernel density fits our data, and also a good method for searching through the space of all possible combinations of kernels.  Our goal in FewKDE is not to create an algorithm that will converge to the best possible solution, but instead one that gives a reasonably good approximation all of the time.  In other words, it is the general robustness of the algorithm which we wish to preserve, not extreme accuracy in a few particular cases.

The likelihood of an kernel estimate $K$ should be determined by the chance that we will see the data points, assuming that the estimate is right.  The likelihood should also take into account the weight of each data point.  One way to do this, for a Monte Carlo $i$, is:

\begin{equation}
\label{eqn:thingy}
{\cal L}_i = p(\vec{x_i}|K)^{w_i}
\end{equation}

\begin{equation}
\label{eqn:thingy2}
{\cal L}_i = K(\vec{x_i})^{w_i}
\end{equation}

Because Monte Carlo points can be treated as independent events,

\begin{equation}
\label{eqn:thingy2}
{\cal L} = \prod_i {\cal L}_i = \prod_i K(\vec{x_i})^{w_i}
\end{equation}

We now define the quality of a kernel estimate $K$ to be the logarithm of the likelihood, given by
\begin{equation}
\label{eqn:Likelihood}
\log {\cal L} = \sum_i{w_i \log K(\vec{x}_i)},
\end{equation}
where $K(\vec{x}_i)$ is the number obtained by evaluating the kernel estimate at the $i^{\text{th}}$ Monte Carlo point.

The choice of kernel estimator is simplified by noting an important property of the distributions of interest to high energy physicists studying high-$p_T$ collider physics.  Distributions derived from four-vector quantities (energy, mass, and two angular variables mentioned in Sec.~\ref{sec:The problem}) of final state objects in high-$p_T$ collider physics are rather featureless: by this, it is meant that there are no sharp slopes, and also that there is usually only one local extremum (not counting the sharp cutoffs at the ends of the distribution).  As a result, it is our contention that the distribution can be approximated satisfactorily by the sum of at most five Gaussians.  By limiting the number of kernels used, the resulting distribution will be more practical to compute(see Subsection~\ref{subsec:Time}).  

Five kernels are placed, with means drawn from a multivariate Gaussian distribution with the mean and covariance of the Monte Carlo sample, with variances drawn from a uniform distribution ranging between 0.8 and 1.2 times the variances of the sample, and with individual kernel weights drawn from a uniform distribution between 0 and 1.  The maximization of Eq.~\ref{eqn:Likelihood} is performed using the ``amoeba'' algorithm from Numerical Recipes; the initial displacements are taken to be 10\% of each value in each direction.  The procedure is repeated ten times to enhance the odds of finding a global maximum.

\subsection{Time}

The computational time of the construction of the density estimate scales as the number of kernels times the total number of parameters times the number of Monte Carlo points, which is ${\cal O}(n) \times {\cal O}(n \, d) \times {\cal O}(\nMC) = {\cal O}({n}^2 \, d \, \nMC)$.  The number of kernels $n$ is five, a constant.  The computational time of the construction of $K$ thus scales as ${\cal O}(d \, \nMC)$.  In our problems the dimensionality $d \ll 10$, so the dependence on $d$ can be ignored.  The scaling with the number of Monte Carlo points $\nMC$ is ${\cal O}(\nMC)$; given that each Monte Carlo point must be touched at least once, it is not possible to do better.

\begin{figure}[htp]
%\includegraphics[angle = 270, width=3.0in]{timegraph}
\label{fig:timegraph}
\end{figure}

In the standard and adaptive non-parametric kernel estimates, the time required to evaluate the density at all $\nMC$ points scales as ${\cal O}({\nMC}^2)$.  With \FewKDE, the time required at each point is proportional to the number of kernels, which is constant, so that the total time required to evaluate the \FewKDE\ density at all $\nMC$ points scales as ${\cal O}(\nMC)$.  This is again the best that can be achieved. 

\section{Examples}

Four different distributions were used to compare the effectiveness of the three estimators.  In all the examples, red represents FewKDE, green represents the standard kernel density estimate, and blue represents the adaptive kernel density estimate.

\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{box1}
\caption{\ Comparison of a uniform parent distribution (black) to various estimates: \FewKDE (red), Standard kernel density estimate (green), and Adaptive kernel density estimate(blue). }
\label{fig:1Dbox}
\end{figure}

Fig.~\ref{fig:1Dbox} shows that none of the three estimators is a particularly good fit for a uniform parent distribution.

\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{gausses}
\caption{\ Same as ~\ref{fig:1Dbox}, but for a Gaussian parent distribution.}
\label{fig:1Dgauss}
\end{figure}

\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{breit_wigner1}
\caption{\ Same as ~\ref{fig:1Dbox}, but for a Breit-Wigner parent distribution.}
\label{fig:1Dbw}
\end{figure}

Figs.~\ref{fig:1Dgauss} and ~\ref{fig:1Dbw} shows that the data is best fit by FewKDE.  In general, both the adaptive and standard approach tend to be a bit less sharply peaked than the actual data for both this and the Breit-Wigner distribution.

\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{exp_decay1}
\caption{\ Same as ~\ref{fig:1Dbox}, but for a Exponential Decay parent distribution.}
\label{fig:1Dexp}
\end{figure}

Fig.~\ref{fig:1Dexp} illustrates the problems caused by sharp cutoffs in the standard and adaptive approach.  FewKDE matches the parent distribution almost perfectly throughout the entire domain, while both the standard and adaptive kernel densities fall off close 0, even though the parent distribution actually peaks there.

For the 3-dimensional plots, the figures depict surfaces and not curves, which makes it difficult to overlay them in the same graph.  For these figures, we have made 2x2 grids in which the actual distribution and the FewKDE, standard, and adaptive approaches are plotted.  Each plot is shown from two different viewpoints: one with a viewer along the x-axis, and one with the viewer along the y-axis.

\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{box2D3}
\caption{\ Four 3-D graphs viewed with the Y-axis coming out of the page.  $Upper left:$ Uniform parent distribution.  $Upper right:$ FewKDE. $Lower left:$ Standard kernel density estimate. $Lower right:$ Adaptive kernel density estimate.}
\label{fig:2DboxY}
\end{figure}
\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{box2D2}
\caption{\ Same as ~\ref{fig:2DboxY}, but with X-axis coming out of the page.}
\label{fig:2DboxX}
\end{figure}
\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{gauss2D1smooth}
\caption{\ Same as ~\ref{fig:2DboxY}, but for a Gaussian parent distribution.}
\label{fig:2DgaussY}
\end{figure}
\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{gauss2D2smooth}
\caption{\ Same as ~\ref{fig:2DgaussY}, but with X-axis coming out of the page.}
\label{fig:2DgaussX}
\end{figure}
\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{breit_wigner2D1}
\caption{\ Same as ~\ref{fig:2DboxY}, but for a Breit-Wigner parent distribution.}
\label{fig:2DbwY}
\end{figure}
\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{breit_wigner2D2}
\caption{\ Same as ~\ref{fig:2DbwY}, but with X-axis coming out of the page.}
\label{fig:2DbwX}
\end{figure}
\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{exp_decay2D1}
\caption{\ Same as ~\ref{fig:2DboxY}, but for a Exponential Decay parent distribution.}
\label{fig:2DexpY}
\end{figure}
\begin{figure}[htp]
%\includegraphics[angle = 270, width=3in]{exp_decay2D2}
\caption{\ Same as ~\ref{fig:2DexpY}, but with X-axis coming out of the page.}
\label{fig:2DexpX}
\end{figure}

\section{Summary}

\FewKDE\ is a non-parametric kernel density estimator whose evaluation time scales as ${\cal O}(\nMC)$ with the sample size $\nMC$.  The slow evaluation of the standard kernel estimator, in which one kernel is placed for every point in the sample, is an oft-cited reason for preference for the neural network, which offers very fast evaluation.  \FewKDE\ directly addresses this concern by compacting the representation of the kernel estimate into a few kernels, whose subsequent evaluation is as fast as that of a neural network.

In addition, \FewKDE\ explicitly handles the types of hard physical boundaries most common in high energy physics applications by using the extrema of the points in the Monte Carlo sample to define a region of finite support.  Examples of this are shown in Figs.~\ref{fig:fewKDEuniform} and~\ref{fig:fewKDEexp}.

\acknowledgments

Sang-Joon Lee, David Scott, Paul Padley, and Hannu Miettinen (Rice University), and Lasse Holmstr\"{o}m (University of Helsinki) have contributed useful insights and helpful dialog on this article and on related issues.

\bibliography{fewKDE}

%===============================================================
\end{document}

