[CRAB]

jobtype = cmssw
scheduler = glite

use_server=1
#server_name = cern

[CMSSW]

lumi_mask = Cert_132440-133511_StreamExpress_Commissioning10-Express_DQM_JSON.json
total_number_of_lumis = -1
lumis_per_job = 200



### The data you want to access (to be found on DBS)
#dbs_url = https://cmsdbsprod.cern.ch:8443/cms_dbs_ph_analysis_02_writer/servlet/DBSServlet

datasetpath= /MinimumBias/Commissioning10-Muon_skim-v8/RAW-RECO 

#/MinBias/Spring10-START3X_V26_S09-v1/GEN-SIM-RAW
#/MinimumBias/Commissioning10-Muon_skim-v8/RAW-RECO 
#/MinBias/Spring10-START3X_V26A_356ReReco-v1/GEN-SIM-RECO

### The ParameterSet you want to use
pset=TauSkimPat_cfg.py

### Splitting parameters by lumisec
###lumi_mask=rpc_ls.json
###total_number_of_lumis = -1
###lumis_per_job = 200
#split_by_run = 1

#total_number_of_events=-1
#events_per_job = 100000
### equivalent to 5 pb-1 for ZMuMu@ 7 TeV
#number_of_jobs = 5

### The output files (comma separated list)
output_file = patTupleMuSkim.root

[USER]

### If CRAB.server_mode = 1
eMail = Letizia.Lusito@cern.ch

### If CRAB.server_mode = 1
### To specify the percentage of finished job in a task, corresponding
### to when the notification email will be sent to you by the server default is 100%
#thresholdLevel = 100

ui_working_dir= /afs/cern.ch/user/l/lusito/scratch0/Tau356/CMSSW_3_5_6/src/TauAnalysis/Configuration/test/PatDataMuonSkimJSON_B

return_data = 0

### To copy the CMS executable output into a SE
copy_data = 1

### if you want to copy data in a "official CMS site"
#storage_element = T2_IT_Bari

### if you want to copy your data to your area in castor at cern
### or in a "not official CMS site" you have to specify the complete name of SE
storage_element=srm-cms.cern.ch
#storage_element =cmsse01.na.infn.it

### this directory is the mountpoint of SE
storage_path=/srm/managerv2?SFN=/castor/cern.ch/
#storage_path=/srm/managerv2?SFN=/dpm/na.infn.it/home/cms
### directory or tree of directory under the mountpoint
user_remote_dir = /user/l/lusito/Data356/PatDataMuonSkimJSON_B

### This is the storage port number. Default is 8443
#storage_port = 8446

### To specify the version of srm client to use. 	
srm_version = srmv2

[GRID]

### To change the CMS-broker RB/WMS to be used. The ones available for CMS
### are "CERN" and "CNAF"
rb = CERN

### CMS myproxy server, to proxy delegation
proxy_server = myproxy.cern.ch
virtual_organization = cms

### To specify  VOMS role and/or group
#role = superman
#group = superheros

### To specify a cpu time and wall_clock_time(=real time) in minutes
#max_cpu_time = 60
#max_wall_clock_time = 60

### To manage White/Black lists: For discovery, please use http://cmslcgco01.cern.ch:8001/
### Use the dns domain (eg fnal, cern, ifae, fzk, cnaf, lnl,....) or the CMSSite name
### T1_US_FNAL. Only Tier2 centers have resources dedicated to user analysis.
### All the storage/computing elements (SE/CE) contained in the strings (comma separated list)
### will/will not be considered for submission.
### SE Black List:
#se_black_list = 
### SE White List
#se_white_list =
### CE Black List:
# ce_black_list = indiacms, ufl
### CE White List:
#ce_white_list =

[CONDORG]

# Set this to condor to override the batchsystem defined in gridcat.
#batchsystem = condor

# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch ==
\"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
#globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))


