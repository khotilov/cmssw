\newcommand{\docnumber}{}
\documentclass[draftmode]{memarticle}
\usepackage{hyperref}

\RCS $Revision: 0.2$

\newcommand{\doctitle}%
  {Strategy for a Common Physics Software Infrastructure}

\newcommand{\shorttitle}%
 {\doctitle}

\newcommand{\authors}%
  { Stefano Argir\`o, Luca Lista~\footnote{editor}, Yana Osborne }

\hypersetup%
  { pdfauthor={}%
  , pdftitle={\doctitle}%
  , pdfkeywords={}%
  , pdfsubject={}%
  }

\tightlists

\begin{document}
\topmatter

\chapter{Introduction}

This documents aims at defining the basic requirements 
to establish a Software Infrastructure for Physics Data Analysis.

\chapter{Physics Analysis Patterns}

CMS Analysts come from a large variety of different experiences.
Different people, analyzing different analysis channels will require
very different usage of the data for their analysis.
For this reason, the Physics Analysis Software Infrastructure has to provide
a good level of flexibility and generality to accommodate many
different uses.
At the same time, the patterns provided by the Physics Analysis 
software infrastructure must avoid anarchic proliferation of
software tools that would become unmanageable. 
Software written for data analysis
brings into CMS system very valuable experiences. 
Reusing the software produced in a specific context for other
purposes, and integrating it in official software releases
must be as easy as possible.  

The learning curve for newcomers should be as short as possible.
Learning how to write a simple analysis application 
should require a small amount of time in order to introduce
newcomers quickly and effectively. ``How-to web'' pages should
be provided in connection to software releases.

\section{Analysis Modularity and Use of the Framework}

Physics Analysis proceeds via subsequent steps. ``Building blocks''
are identified, and more complex objects are built on top of
them. For instance, the search for 
$H\rightarrow Z Z\rightarrow \mu^+\mu^-\mu^+\mu^-$ will require:
\begin{itemize}
\item{} identifying muon candidates;
\item{} reconstructing Z candidates starting from muon candidates;
\item{} reconstructing Higgs candidates starting from Z candidates.
\end{itemize}
This process clearly identifies three products: muon candidates,
Z candidate, Higgs candidate, and three processes to reconstruct 
them. These are well mapped into three Framework modules
(producers) that add in the Event three different products
(the candidates collections). 

Using the Framework properly for analysis enhances modularity 
and allows to separate analysis processes into single units
that can be reused for different applications.

\section{Analysis Products as Software Deliverables}

Experience from past experiments shows that it is a successful
practice to define Physics Analysis Deliverables that can be
reused for different analyses in different contexts. 
A deliverable should consist of two components:
\begin{itemize}
\item{} a set of Event Data Products (e.g.: collections of
candidates)
\item{} a set of Framework modules to produce the specific products
\end{itemize}
Examples could be selections of muons or electrons of different 
efficiency {\em vs} purity, composite candidates from decay
chains (e.g.: $Z\rightarrow \mu^+\mu^-$, $B_s\rightarrow J/\psi\phi$,
$J/\psi\rightarrow e^+e^-, \mu^+\mu^-$, etc.), but also
event-based information like missing $E_t$, etc. that can be
put into the Event for access by modules downstream the 
event processing path.

A ``Deliverable'' (i.e.: a set of event products and the modules 
to generate it) can be developed initially for specialized cases, 
then can be generalized, officially released and adopted in general 
for wider usage.
Of course, not all analysis modules used in any channels are due to become
official Deliverables.

Widely used analysis products need to be characterized in term of
performances (could be efficiency, misidentification rate, resolution,
sets of correction to apply to Monte Carlo, etc.) that can be essential 
for use in concrete analyses, and should be provided in a controlled,
documented and official way. 
This requires organizing Physics Tools Groups active
on the most important and widely used Analysis 
Software Tools, committed to provide 
official Physics Analysis Deliverables.

This approach has a number of benefits in a very large community as
an LHC experiment:
\begin{itemize}
\item{} Analysis using specific products will be pushed to use
official ``building blocks''. This provides the analysis with  
reliability of the adopted tools, easy access to product
characterization, which are important for performance and systematics studies and robustness of the final result.
\item{} People working on Physics Tools will feel motivated
because their products will be part of many official analyses, and could possibly become tools for discoveries.
\item{} Physics Tools as part of official software 
releases will provide two types of components:
one or more products available in the Event, and
one or more producer modules that able to produce such products.
When possible and required, the products will be accessible 
in persistent format; otherwise, will be
easily reproducible by the provided producers.

\item{} Performing new analyses will be effective, without the need
to restart from scratch, since the basic ``building blocks'' will
be readily available. 
\end{itemize}

Physics Software Tools must be well documented with examples and
references available and organized in web pages, in order to minimize
the learning curve for new users.

\chapter{Physics Software Model}

This section will discuss some of the software requirements to
satisfy a general set of typical patterns of data
processing for Physics Analysis.

\section{Patterns of Data Access}

A typical analysis will proceed in subsequent steps of
data reduction. Those can proceed either centrally
at Tier-1 or Tier-2~\cite{CMSCM}, starting from one or few selected
main streams, in a scheduled and organized way, 
or ``locally'' at Tier-2, Tier-3 in a more ``user driven'' way.

\subsection{Events Skimming}

Data reduction means the following things:
\begin{itemize}
\item{} select a subset of the events satisfying a specified preselection;
\item{} select a subset of the information (i.e.: Event Data Products)
to be stored to the output collection;
\item{} possibly, add more Event Data Product that could 
be the output of CPU-expensive processing steps that the user
doesn't want to repeat in further analysis processing steps.
\end{itemize}
Such processes can proceed in a different number of steps, according
to Physics Groups needs, and different ``granularities'' according
to needs and organizations of different groups and different analysts.

The Software Infrastructure should leave freedom to choose the most
efficient strategy for each analysis channel. Once the strategy is defined, 
the centralized part of the organization of data processing
has to proceed in an organized and controlled way.

We give the name ``skims'' to the subsequent steps of data reduction.
Performing deeper level of skims from a centrally produced skim can be
referred to as sub-skimming or re-skimming.

\subsection{Event Directories or Shallow Copies}

Fast event access can be performed using a concept known 
as {\em event directories} or, in a more general way, as 
{\em shallow copies}. This has the benefit of categorizing
events into selected collections, saving the disk space 
required to host {\em deep copies}. Those
data structures should be accessible with the same interface
as normal event collections. Event Directories or Shallow Copies
require the implementation of the concept of pointer to events in 
external data files. 

It would be desirable to manage Shallow Copy collections with the
same functionality as normal {\em deep copy} skims, i.e.: add 
selected component in extended collections, though ``borrowing''
the content of the rest of the event from pointed events, possibility
to perform re-skimming.

\section{A Flexible Event Model}
\label{sec:ntuples}
The data content of the Event should be extremely flexible.
Analysis users tend to produce ntuple (more precisely, root-tree) dumps
that try to replace the Event Data Model if they don't find
in the Framework the required data access performance, both in term
of speed and flexibility of the content.

We should aim at avoiding large ntules dumps as much as we can. CMS Event Data
Model is based on POOL/ROOT I/O; interactive inspection capability is built-in 
to the system, so ntuple-like content should naturally and easily become
part of the Event Data Model itself. The interactive browsability of the
Event data files can actually provide in the Event Data Model itself with  all
the capabilities formerly requested to the ntuple. 

In order make ntuple dumps external to the EDM unattractive, 
it should be possible to the user
to add to the Event either simple objects, even native types 
(\cppcode{int}, \cppcode{float}, ...), and collections 
of reconstructed particle candidate defined by the user 
using the same interface as official collections.

If this goal is achieved, a single and consistent data reduction
process will lead step by step from DST to AOD to TAG, and eventually to
ntuples and histograms, without need to change technology when 
moving from Event-based content to User-defined analysis object content.

This would also lead to a significant reduction of overall disk
space needs for the experiments, and funding agencies will
appreciate. 

\section{Package Organization}

The organization into packages of the classes needed
in the Physics Software Infrastructure have to be 
devised carefully in terms of dependencies, in order
to allow several kinds of access in an efficient manner.
At least interactive access via ROOT, 
with or without loading (light) external libraries,
and batch production mode access should be provided. 
Similar guidelines as for reconstruction products hold
for the Analysis Software as well.

\section{Persistent Data Types}

Persistent data representation should have minimal
dependencies on external libraries, and should not
depend on external services (\cppcode{EventSetup}), in order to
allow a light interactive access.

\section{Analysis Basic Types}

This section describes the basic data types
that can be required by the majority of Physics Analysis 
software.

The aim of the Analysis Software Infrastructure
should be to provide all or almost all the required
functionality for analysis without the need for
the user to define custom data types. 
In this way non-expert analysts don't need 
to define new classes, other than modules
that will contain the algorithmic part.
Nevertheless, defining new data types should be 
allowed, as natural extension of a data analysis in 
a more mature stage, and for advanced analysts, with
good software technology skills. Also, if new types of general interest are
developed, it should be easy to integrate them 
as general tools.

\subsection{Analysis Object Data (AOD)}

Data Analysis requires access to subdetector information.
In Analysis stages that require fast data access, 
it may not be possible to access all the DST information.
The subdetector information should be available for
analysis in a compact way.
The exact definition 
of Analysis Object Data for different subdetectors
may be subject of optimizations and changes in time,
so we may expect an evolution of those data types 
even after the start of data taking.
Navigation from AOD to corresponding DST objects should
be supported.

\subsection{Particle Candidates}
Users would largely benefit from having a uniform access to information
common to all reconstructed particles, either when 
directly reconstructed from detector information 
or from a decay chain. A uniform interface should
provide access to kinematic information (4-momentum, vertex,
possibly error matrices required to perform
geometric-kinematic constrained fits) and navigation
among constituents of decay chains.

Those objects are called {\em Candidates}. Candidates 
can be built in principle on top of either DST or AOD, 
in the sense that they could be built from, and all navignation
into, DST or AOD subsystem objects. For this reason, 
Candidates could be component and should
be an event component that sits on top of them. For most
of the real analysis cases, anyway, it is reasonable to
think to use candidates built on top of AOD, in order
to maximise the anaysis performance. Candidates built
on DST could be useful in a development phase, where AOD
are not yet defined, while prototypes of DST are available.

Particle assignment (i.e.: the particle type for the 
specific assumed hypothesis) could also be important, and
can be considered for inclusion. This could be useful,
for instance, to analyze in a fast way collections of
decay chains containing {\em leptons} identifying which decays
contain {\em muons} and which {\em electrons}.

%Having particle 
%could also allow to create a candidate 
%structure for Monte Carlo information uniform to
%the candidate structure in data, but this is 
%a matter of choice.

A possible approach would be to identify candidate
collections via different candidate types (e.g.: \cppcode{ZCandidate}, 
\cppcode{HiggsCandidate}, etc.). The experience from
Beta toolkit for BaBar experiment~\cite{Beta} shows
that a single class (or better, a single polymorphic
class hierarchy) can be appropriate for all cases,
and gives users a much more uniform ground, 
reduces the number of concepts that have to be
learned, and turns out to be much more effective.
For this reason, we consider a single candidate class to be better
than many candidate classes.
This would also induce access patterns to the
Event Data Model that are more driven by names
than by types (the latter being the case of reconstruction), 
since most of the particle collections
will basically just be \cppcode{vector<Candidate>}.
See also section~\ref{sec:edm}.

Candidate collections need to be persistable, 
in order to avoid re-doing the combinatorial 
analysis that is frequently CPU expensive.

\subsection{User Data}
\label{sec:userdata}
Users need to add their own data to the Event when 
producing reduced data sets (or skims) either to 
persist quantities that require large processing time,
or to produce ntuple-like formats for interactive 
access (see section~\ref{sec:ntuples}).

User Data~\cite{UserData}
can be associated to the whole event, or to event product,
typically but not exclusively, to candidate. 
A typical application could consist in associating 
some isolation quantity to a lepton. In a
simplified example where this variable is just a 
\cppcode{double}, the user may want to plot 
interactively the value of 
the isolation {\em vs} the $p_t$ of the lepton.
This possibility should be supported by 
the Analysis Software Infrastructure in 
a way that is consistent with the general Event Data Model.

User Data are important not only for interactive analysis,
but in subsequents phases of skimming to avoid the need
to perform the same computation multiple times.
The access to User Data in batch should 
also be defined in order to perform effective
batch processing.

\section{Transient Services for Data Analysis}

Performing constrained fits, combining multiple candidates,
computing recalibrated AOD, are tasks that can be done
by transient classes that encapsulate algorithms.
Those algorithms should be implemented in a way consistent
with the overall data model for Physics Analysis, 
in particular with the Candidate class interface.

External services may be required to access 
``physics calibration quantities''
(e.g.: tables for efficiency and
misidentification rates, Monte Carlo correction factors
needed to implement realistic simulation at Candidate
level, etc.).
Those quantities may vary with time according to 
detector conditions, and should be stored in the 
conditions database.
Analyses requiring access to such quantities 
require a fast access to the conditions database
that could also be accessed from Tier-2 and Tier-3 sites.

\section{Reusable Framework Modules}

Framework modules (producers) are required to store
new products in the Event. Typical cases can be
selectors (to store collections of selected data objects 
from a given source collection), composition or 
combinatorial analysis, etc. 
Such modules should be provided in a general way
and should be readily usable by the user without the
definition of new classes, at least for the basic
functionality.

For instance, it should be possible to produce
a collection of Higgs candidate from  
$H\rightarrow Z Z\rightarrow \mu^+\mu^-\mu^+\mu^-$
just instantiating a composition module for the 
decay $Z\rightarrow \mu^+\mu^-$ which has a muon 
candidate collection as source, and a module
for the decay $H\rightarrow Z Z$ which has 
the $Z$ collection produced by the previous 
combinatorial module as source. $H\rightarrow Z Z*$,
with a virtual Z, would be a variation with two 
different $Z$ and $Z*$ collections as sources.
Trivial checks, like avoiding considering 
the same $\mu$ candidate twice in the same $H$ decay
should be avoided automatically by the combiner modules.

\section{Monte Carlo Truth tools}

It would be very helpful to develop Monte Carlo truth 
matching tools that provide HepMC particles at the generator
level associated to reconstructed particle candidates including
also particles reconstructed from a decay chain.
For reconstructed decay chains (e.g.: $Z\rightarrow\mu^+\mu^-$)
requesting the MC truth associated to the $Z$ candidate 
should check the following:
\begin{itemize}
\item check MC matches to $\mu^+$ and $\mu^-$
\item check for a common parent particle
\item check that the common parent has no other daughters
\item return the MC truth associated to the $Z$ if
matches and checks are successful, invalid reference 
otherwise.
\end{itemize}

\chapter{Requirements to the Event Data Model}
\label{sec:edm}
The Analysis Model described above puts a few requirements 
to the Event Data Model (EDM) which would be an addition to the
features presented in the Road Map document~\cite{roadmap}.

\section{Polymorphic Collections in the EDM}
Collections of Candidates should be exchanged among Framework
modules via EDM, and possibly be persisted.

An implementation of Candidate avoiding polymorphism is 
possible, basically implementing all possible required persistent 
references, keeping null values for those not required.
This could result in a significant disk space overhead,
in particular for composite Candidates that would hold 
references to subdetector informations (AOD).

Candidate objects could be organized according to a 
Composite Pattern~\cite{dp}, where Candidates representing
particles with subsequent decays (e.g.: $Z\rightarrow e^+e^-$)
are {\em composite} objects, Candidates representing reconstructed
particles ($e^+$ and $e^-$ in the previous decay) are the {\em leafs}
of the model.
Collections of polymorphic objects are supported by the EDM from version
1.30 of the Road Map document~\cite{roadmap}. However, polymorphic
collections give problems with bare ROOT interactive
browsability. 

The issue of polymorphic Candidate collection has to be 
addressed, and the implications to the user understood
in term of functionality and performances.

\section{Generic Data Products}

Section~\ref{sec:userdata} presented the need for user-defined
quantities to be added to the event persistently. This in order
to replace large ntuple dumps for the final steps of 
interactive data analysis. With this approach, the Event should be
the final persistent support to interactive data analysis. A similar scheme
is also supported in Ref.~\cite{roadmap}.

In order to provide a flexible user definition of 
the Event content, the Event Data Model should provide 
sufficient flexibility in its content.
The simplest example one can imagine is the storage of a few
persistent \cppcode{double} and \cppcode{int} variables
in the event (say, the missing $E_t$, the absolute value 
of the missing $p_t$, and the number of isolated electrons and 
muons $N_e$, $N_\mu$).

To do this, two additional requirements have to be put on 
the current EDM:
\begin{itemize}
\item{} Objects of generic type can be put into the Event. The current
version of the EDM restricts this possibility, and only products 
inheriting from the base class {EDProduct} can be put in the Event. 
This is clearly not possible for native types like \cppcode{int}, 
\cppcode{double} in the previous example, or other simple types like 
Lorentz-Vectors, Spatial points, etc. Defining specific types
(e.g.: \cppcode{UserData<int>}, \cppcode{UserData<double>},
\cppcode{UserData<LorentzVector>}, ...) is possible, but would
lead to one extra level of indirection.

\item{} Multiple products (i.e.: branches) of the same type
can be put in the Event by the same producer. At the moment, the 
same producer can put in the Event multiple products only of
they are all of different types. The reason is that the type is 
used, together with the module label, as product identifier,
namely as ROOT branch name.
This limitation prevents a user to store within the same 
Framework module both $E_t$ and $p_t$, or both $N_e$ and $N_\mu$.
Adding more objects of the same identifier would require specifying
a branch name alternative to the one univoquely identified by the pair 
(object type, module labeler). Alternative branch names would also 
allow compact label that are desirable for a fast interactive analysis,
where users just type the label names from the ROOT CINT interactive prompt.
\end{itemize}

\chapter{Development plan}

The main guidelines for Physics Analysis development plan
are the milestones set for the overall CPT project.
The following are the most relevant for the development of the
Physics Analysis tools:

\begin{itemize}
\item{CPT-208} {\em October 2005}: First version high-level physics algorithms/objects 
using re-engineered EDM/FW.
\item{CPT-210} {\em November 2005}: Demonstration of an analysis application for 
the Physics TDR using the re-engineered EDM/FW. 
\item{CPT-402} {\em March 2006}: Detector and Physics Reconstruction (DST) for the CSA-2006 
\item{CPT-404} {\em June 2006}: Calibration, alignment, analysis and visualization ready for 
CSA-2006
\item{CPT-502} {\em December 2006}: Demonstrate performance of HLT/offline reconstruction, 
alignment, visualization 
\item{CPT-504} {\em February 2007}: Software Complete HLT, reconstruction, simulation 
calibration, alignment, visualization, analysis
\end{itemize}

The following two Sections mainly describes the plan for 
the Analysis Module; Visualization part is described
in Section (\ref{sec:visualization}).

\section{Short-term milestones (2005)}
In order to fulfill milestone CPT-210, we have to proceed fast toward 
a first, even preliminary definition of high level objects (DST), 
in order to start developing a first simplified version of the AOD
and Candidate objects. This allows to start developing examples
of modular analysis. One of the simplest channel, still able to
demonstrate the structure and advantages of a modular analysis
approach using the Framework and EDM, could the the study of
Higgs boson decaying into four leptons. The basic ingredients
are the definition of tracks, and the identification of muons
and/or electrons.

Collections of signal and background event can be converted~\cite{converter} from 
ORCA production into EDM-compiant DST format, on top of which the
new analysis chain can be exercised.

The development of Candidate objects can proceed in parallel.
A prototype using ORCA/COBRA has already been developed~\cite{candroto} with
a limited set of functionality. It can be further optimized, and
ported to an EDM-compliant form. Given the short timescale, Candidate
objects can be first implemented on the basis of DST components, then be
migrated to lighter AOD objects when available. This should not induce 
any change in their structure, given Candidates ``generic'' dependency 
on AOD or DST.

The following short-term plan is still preliminary.
\renewcommand{\labelenumi}{\Roman{enumi}.}
\renewcommand{\labelenumii}{(\arabic{enumii})}
\renewcommand{\labelenumiii}{\arabic{enumiii}.}
\renewcommand{\labelenumiv}{\arabic{enumiii}.\arabic{enumiv}.}
\renewcommand{\theenumi}{\Roman{enumi}}
\renewcommand{\theenumii}{\arabic{enumii}}
\renewcommand{\theenumiii}{\arabic{enumiii}}
\renewcommand{\theenumiv}{.\arabic{enumiv}}

\begin{enumerate}
  \item Definition of High Level Data Types
  \begin{enumerate}
    \item DST (from Reconstruction) \label{enum:dst}
           Depends on:
             plans from Task Forces
             Integration of EventSetup Services (Geom, B-field,...)\\
             due by: {\em October 2005}
    \item AOD \label{enum:aod}
           Minimal info needed for analysis to be identified.
           Depends on:
             DST (\ref{enum:dst}) \\
           plan a first version available for: {\em CPT-210} ({\em November 2005})
    \item Particle Candidate types \label{enum:candidates}
           basic architecture can start shortly (based on the
           prototype developed on CARF, once ported to EDM).
           Realistic use can be tested only with
             AOD (\ref{enum:aod}); could be used initially
           with DST (\ref{enum:dst}) as alternative.\\
           Plan a first version on: {\em September 2005}
    \item User-Defined Data 
           Requires integration of ntuple-like data content within
           the Event Data Model. 
           The feasibility has to be discussed with the EDM team, 
           since it depends on EDM extensions.
    \begin{enumerate}
      \item User Data associated to event (generic ntuple branches)\label{enum:userdataev}. \\
           would be desirable by: {\em October 2005}            
     \end{enumerate}
  \end{enumerate}
  \item Common analysis Framework modules \label{enum:commonmodules}
       Should be taken from real analyis examples. One of the first
       examples could be $H\rightarrow 4 \ell$. \\
       Plan a first version for: {\em CPT-210} ({\em November 2005}).
       First modules could be:
  \begin{enumerate}
    \item Combinatorial analysis
    \item Products Selectors
  \end{enumerate}
  \item Examples of analysis within the new Framework
       will evolve continuously.
       Can start with
         first implementations of Particle
         Candidates (\ref{enum:candidates}), 
         first versions of common combinatorial modules
         (\ref{enum:commonmodules}). 
       Should be integrated with documentation and tutorials.\\
       Plan a first version available for {\em CPT-210}({\em November 2005})
  \begin{enumerate}
  \item Documentation:
       should provide easy-to-use examples, how-to web pages.
       It is essential for newcomers and for the success of
       the model. Should be an ongoing effort.
  \end{enumerate}
\end{enumerate}

\section {Medium-term milestones (2006)}

The following milestones are less precisely defined
that the short-term ones from the previous section.

\begin{enumerate}
  \item Definition of High Level Data Types
  \begin{enumerate}
     \item User-Defined Data 
    \begin{enumerate}
      \item User Data associated to other Analysis Products:
               Identify which kind of container;
               identify which root-interactive use patterns 
               (es: plot isolation vs lepton $p_t$). \\
           Plan for initial implementation: {\em January 2006}, 
           would be desirable for {\em CPT-402} ({\em March 2006})
    \end{enumerate}
    \item Tag "bits" for fast selection: 
           Could technically be the same format as:
             Event User Data (\ref{enum:userdataev}). \\
          Would be desirable for {\em CPT-402} ({\em March 2006})
  \end{enumerate}
  \item Integration of existing manipulation tools
  \begin{enumerate}
    \item Constrained mass/vertex fitters.\\
       Plan a first version available for: {\em January 2006}
  \end{enumerate}
  \item Event Directory:
       equivalent to "shallow copy" collections. Requires the
       implementation and management of pointers to events from
       different files.). \\
           Plan for {\em CPT-404} ({\em June 2006})
\end{enumerate}


\section{Visualization}
\label{sec:visualization}

This section describes visualization milestones and
deliverables for the period from mid 2005 to mid 2006. These have been
determined by an iterative process from the requests and feedback of
the SW and PRS groups. This section does not cover existing features
of the current CMS visualization tools (e.g. IGUANA-based
applications) nor does it supplant the design, reference and user
documentation.

In the second half of 2005 the visualization development will be
concentrating on creating a solid foundation upon which
the currently available interactive applications
can be migrated to the new EDM and framework. This will include
development of the visualization sub-framework, integration of new
generic graphics toolkits (e.g. ROOT) and migration of the existing
visualization components to the newly ported to the EDM/FW CMS
software. First functional applications will be deployed in the
context of the Cosmic Challenge to provide a detector, event and
histogram displays for the detector groups.

In parallel with the new development based on the new EDM and
framework currently available ORCA and OSCAR based applications will
be maintained to allow the PRS groups continue their work on the
Physics TDR especially what concerns creating a picture gallery for
the volume II.

In 2006, the main focus will be very much on functionality needed for
the CSA-2006. The solid IGUANA foundations will be exploited to
broaden the applications provided by IGUANA to the PRS groups,
particularly:

\begin{enumerate}
  \item Visualization for simulation, fast simulation, detector
  geometry description and Geant4;
  \item Visualization for detector and physics reconstruction
  including visualization of the high level objects (DST), the
  simplified version of the AOD and Candidate objects;
  \item Visualization for on-line: detector display, event display and
  histogram display.
 \end{enumerate}

The following milestones
and level 3 tasks have been agreed upon:

\begin{enumerate}
  \item {\em June - July 2005} ORCA-based detector display and event
  display complete for the Physics TDR.

  \item {\em June - August 2005} Ongoing development toward
  integration of a detector and event display with new EDM/framework
  and a histogram display with DQM software (ROOT+IGUANA based GUI for
  DQM histograms).

  This task assumes that July version of CMSSW will include basic
  functionality of the framework needed for interactive event access,
  the event data (rechits and digis) for tests produced in the new
  format, foundation utilities, basic services, generators, geometry
  services, interfaces to DDD for simulation and reconstruction, EvF
  and low-level detector data defined, documented and released,
  generally all the infrastructure is put in place: Web pages, release
  procedures and release plan.

  \item {\em CPT-203: September 2005} First version of event
  processing applications for Cosmic Challenge

  Rudimentary detector and event display based on new EDM and
  framework demonstrating an ability to load and display detector
  geometry and low-level event data.

  Demonstrate communication with an EventProcessor, access to rec hits
  and digis, use of the geometry service. Reading of the rechits and
  digis will be done from a file produced by other applications.

  This task assumes that August version of CMSSW will include complete
  detector-specific interfaces to DDD, local-to-global coordinates
  conversion from a geometry service, EvF and low-level detector data,
  digis, rechits, raw2digi converters, more complete framework
  functionality, all SW ported which is needed for digitization.

  \item {\em September - October 2005} Picture gallery for the Physics
  TDR.

  \item {\em October - November 2005} Deployment of detector display,
  event display and histogram display (ROOT+IGUANA based GUI for DQM
  histograms) for Cosmic Challenge.

  \item {\em CPT-211 Dec 05} Software and Detector procedures ready
  for Cosmic Challenge
  Detector display, event display and histogram display ready for
  Cosmic challenge.

  \item {\em January - February 2006} Debugging of the software for
  validation of the pre-challenge production.
  Detector display, event display and histogram display for simulation
  and digitization of pre-challenge production of CSA-2006
  samples. (Depends on CPT-401).

  \item {\em March - May 2006} Preparation for CSA-2006.
  Detector display, event display and histogram display for
  reconstruction of pre-challenge production of CSA-2006
  samples. (Depends on CPT-403).

  \item {\em CPT-405: June 2006} ...analysis and visualization ready
  for CSA-2006
  Detector display, event display and histogram display ready for
  CSA-2006. Complete the addition of key missing functionality needed
  for CSA-2006.
 \end{enumerate}

% ----------------------------------------------------------------
\begin{thebibliography}{99}

\bibitem{CMSCM} C.~Grandi, D.~Stickland, L.~Taylor, \textit{ed.},
\textbf{The CMS Computing Model}, {\em CERN-LHCC-2004-035/G-083}, {\em CMS~Note 2004-031}.

\bibitem{Beta} R.J.Jacobsen, 
\textbf{BaBar's Beta Toolkit}, Proceedings of CHEP97, Berlin, April 7-11, 1997.

\bibitem{UserData} G.De Nardo, L.Lista,
\textbf{User Defined Data in the New Analysis Model of the BaBar Experiment},
proceedings of IEEE 2003 Nuclear Science Symposium and Medical Imaging Conference,
{\em SLAC-PUB-11099}.

\bibitem{roadmap} ``The EDM design team'',
\textbf{CMS Core Software Re-engineering Roadmap}, Rev. 1.30,
{\em http://www.uscms.org/LPC/lpc\_offl/RoadMap.pdf}

\bibitem{dp} E. Gamma, R. Helm, R. Johnson, J. Vlissides
\textbf{Design Patterns}, Addison-Wesley. 1995. {\em ISBN 0-201-63361-2}.

\bibitem{converter} L.Lista, DST conversion:
{\em \special{html:<a href="http://people.na.infn.it/~lista/dst/">}http://people.na.infn.it/$\sim$ lista/dst\special{html:</a>}/}

\bibitem{candroto} F.Fabozzi, AOD/Candidates prototype:
presentation at APROM meeting, 22 October 2004: 
{\em http://agenda.cern.ch/fullAgenda.php?ida=a044194}\\
L.Lista,  AOD/Candidates prototype:
presentation at RPROM meeting, 26 July 2004:
{\em http://agenda.cern.ch/fullAgenda.php?ida=a043344}

\end{thebibliography}

% ----------------------------------------------------------------
\end{document}
