\documentclass[a4paper]{jpconf}
\usepackage[dvipdfm]{graphicx}
\usepackage[dvipdfm]{hyperref}
\usepackage{subfigure}
\usepackage{mediabb}
\begin{document}
\title{CMS data quality monitoring: systems and experiences}
\author{L Tuura$^1$, A Meyer$^{2,3}$, I Segoni$^3$, G Della Ricca$^{4,5}$}
\address{$^1$ Northeastern University, Boston, MA, USA}
\address{$^2$ DESY, Hamburg, Germany}
\address{$^3$ CERN, Geneva, Switzerland}
\address{$^4$ INFN Sezione di Trieste, Trieste, Italy}
\address{$^5$ Universit\'a di Trieste, Trieste, Italy}
\ead{lat@cern.ch, andreas.meyer@cern.ch, ilaria.segoni@cern.ch, giuseppe.della-ricca@ts.infn.it}

\begin{abstract}
In the last two years the CMS experiment has commissioned a full end to end
data quality monitoring system in tandem with progress in the detector
commissioning.  We present the data quality monitoring and certification
systems in place, from online data taking to delivering certified data sets
for physics analyses, release validation and offline re-reconstruction
activities at Tier-1s.  We discuss the main results and lessons learnt so far
in the commissioning and early detector operation.  We outline our practical
operations arrangements and the key technical implementation aspects.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Overview}

Data quality monitoring (DQM) is critically important for the detector and
operation efficiency, and for the reliable certification of the recorded data
for physics analyses.  The CMS experiment at CERN's Large Hadron
Collider~\cite{cms_tp} has standardised on a single end-to-end DQM chain
(Fig.~\ref{fig:overview}).  The system comprises:

\begin{itemize}
  \item tools for the creation, filling, transport and archival of histogram
    and scalar monitor elements, with standardised algorithms for performing
    automated quality and validity tests on value distributions;
  \item monitoring systems live online for the detector, the trigger, and the
    DAQ hardware status and data throughput, for the offline reconstruction
    and for validating calibration results, software releases and simulated
    data;
  \item visualisation of the monitoring results;
  \item certification of datasets and subsets thereof for physics analyses;
  \item retrieval of DQM quantities from the conditions database; %FIXME: historic dqm
  \item standardisation and integration of DQM components in CMS software releases;
  \item organisation and operation of the activities, including shifts and tutorials.
\end{itemize}

\begin{figure}[!b]
\begin{center}
\includegraphics[width=.60\textwidth]{Snippets/DQM_end_to_end}
\end{center}
\caption{\label{fig:overview}DQM system overview.}
\end{figure}

The high-level goal of the system is to discover and pin-point errors early,
with sufficient accuracy and clarity to reach good detector and operation
efficiency.  Toward this end, standardised high-level views distill the body
of quality information into summaries with significant explaining power.
Operationally CMS partitions the DQM activities in online and offline to {\em
  data processing}, {\em visualisation}, {\em certification}, and {\em
  signoff}, as illustrated in Fig.~\ref{fig:systems} and described further in
subsequent sections.  The CMS DQM supports mostly automated processes, but use
of the tools is also foreseen for the interactive and semi-automated data
processing at the CAF analysis facility~\cite{caf}.

\begin{figure}[!b]
\begin{center}
\subfigure[Online DQM system.]{\label{fig:online}%
  \includegraphics[height=.47\textwidth]{Snippets/DQM_online}}
\hspace{1in}
\subfigure[Offline DQM system.]{\label{fig:offline}%
  \includegraphics[height=.47\textwidth]{Snippets/DQM_offline}}
\caption{\label{fig:systems}DQM workflows.}
\end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Online DQM system}
\subsection{Data processing}

As illustrated in Fig.~\ref{fig:online}, the DQM applications are operated in
the online as an integral part of the rest of the event data processing at the
cluster at CMS Point-5.  DQM distributions are created at two different
levels, {\em high-level trigger filter units} and {\em data quality monitoring
  applications}.

The high-level trigger filter units produce a limited number of histograms at
up to 100~kHz.  The histogram monitor elements are delivered from the filter
units to the storage managers at the end of each luminosity section.
Identical histograms across different filter units are summed together and
sent to a storage manager proxy server, which saves the histograms to files
and serves them to DQM consumer applications along with the events.

The data quality monitoring applications receive event data and trigger
histograms from a DQM monitoring event stream from the storage manager proxy
at the rate of about 10-15~Hz, usually one application per subsystem.  Events
are selected for the stream by applying DQM group specified trigger path
selections.  Each DQM application requests data specifying a subset of those
paths as a further filter.  There is no special special event sorting or
handling, nor any guarantee to deliver different events to parallel DQM
applications.  The DQM stream provides only raw data products and only on
explicit request additional high level trigger information.

Each application receives events from the storage manager proxy over HTTP and
runs its choice of algorithms and analysis modules and generates its results
in the form of {\em monitoring elements}, including meta data such as the run
number and the time the last event was seen.  The applications re-run
reconstruction according to the monitoring needs.  The monitor element output
includes reference histograms and quality test results.  The latter are defined
using a generic standard quality testing module, and are configured via an XML
file.

\subsection{Visualisation}

All the result monitor element data including alarm states based on quality
test results is made available to a central DQM GUI for visualisation in real
time~\cite{dqm_gui_09}, and stored to a ROOT file~\cite{root} from time to
time during the run.  At the end of the run the final archived results are
uploaded to a large disk spool on the central GUI.  There the files are merged
to large ones and backed up to tape.  The automatic certification summary from
the online DQM step is extracted and uploaded to the run registry and on to
the condition database (see section~\ref{certification}), where it can be
analysed using another web-based monitoring tool, WBM~\cite{wbm}.  Several
months of recent DQM data is kept on disk availabe for archive web browsing.

\subsection{Operation}

Detector performance groups provide the application configurations to execute,
with the choice of conditions, reference histograms and the quality test
parameters to use and any code updates required.  Reviewed configurations are
deployed into a central replica {\em playback integration test system}, where
they are first tested on recent data for about 24 hours.  If no problems
appear, the production configuration is upgraded.  This practice allows CMS to
maintain high quality standard at reasonable response time, free of artificial
dead-lines.

The central DQM team invests significantly in three major areas: 1)~to
integrate and standardise DQM processes, in particular to define and enforce
standard interfaces, naming conventions and look and feel of summary level
information; 2)~to organise shift activities, maintain sufficiently useful
shift documentation, and train people taking shifts; and 3)~to support and
consult subsystem DQM responsibles and physicists using the DQM tools.

All the data processing components, including the storage manager proxy, the
DQM applications and the event display, start and stop automatically under
centralised CMS run control~\cite{runcontrol}.  The DQM GUI and WBM web
servers are long-lived server systems which operate independent of the run
control.  The file management on the DQM GUI server is increasingly automated.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Offline DQM systems}
\subsection{Data processing}

As illustrated in Fig.~\ref{fig:offline}, numerous offline workflows in CMS
involve data quality monitoring: Tier-0 prompt reconstruction,
re-reconstruction at the Tier-1s and the validation of the alignment and
calibration results, the software releases and all the simulated data.  These
systems vary considerably in location, data content and timing, but as far as
DQM is concerned, CMS has standardised on one standard two-step process for
all these activities.

In the first step the histogram monitor elements are created and filled with
event information, and stored as ``run products'' along with the processed
events to the normal event data output files, called EDM files.  When the CMS
data processing systems merge output files together, the histograms are
automatically summed together to form the first partial result.

In a second ``harvesting'' step, run at least once at the end of the data
processing and sometimes periodically during the data processing, the
histograms are extracted from the EDM files and summed together across the
entire run to yield full event statistics on the entire dataset.  The
application also obtains detector control system (DCS, in particular
high-voltage status) and data acquisition (DAQ) status information from the
offline condition database, analyses these using detector-specific algorithms,
and may create new histograms such as high-level detector or physics group
summaries.

The final histograms are used to {\em calculate efficiencies} and {\em checked
  for quality}, in particular compared against reference distributions.  The
harvesting algorithms also compute the preliminary automatic {\em data
  certification decision}.  The histograms, certification results and quality
test results along with any alarms are output to a ROOT file, which is then
uploaded to a central DQM GUI web server as on online.

The key differences between the various offline DQM processes are in content
and timing.  The Tier-0 and the Tier-1s re-determine the detector status on
real data using full event statistics and full reconstruction, adding
monitoring for physics objects.  The Tier-0 does so at $\delta t$ of a couple
of a day or two whereas the Tier-1 re-processing takes from days to weeks.  On
CAF the alignment and calibration validation times vary from hours to days and
the monitored quantities apply to these workflows; as the CAF activity is
still very much seeking form, the DQM details are accordingly sketchy.  The
validation time range on simulated data is proportional to the sample
production times, and can vary from anywhere from under a day for release
validation to weeks on large monte carlo samples.  The validation of simulated
data differs from real data in that entire datasets are validated at once,
rather than runs.

\subsection{Visualisation}

CMS provides one DQM GUI web server instance per offline activity, plus a test
instance for DQM development.  These servers are identical to the server used
on online, and therefore provide a common look and feel for accessing all DQM
data.  From the user perspective, all the CMS DQM data is promptly available
to the entire worldwide for inspection and analysis from one central location.
While the GUI will offer all the capabilities needed for shift and expert use
for all the DQM activities in its final form, it is nevertheless custom-built
for the purpose of efficient interactive visualisation and navigation DQM
results; it is not at all meant to be a completely general analysis tool.

As in the case of online, the DQM results from offline processing are uploaded
to the central DQM GUI server with a large disk spool.  There the result files
are merged to larger size and backed up to the tape; recent data is kept
cached on disk for several months.  The automatic certification results from
the harvesting, called ``quality flags,'' are extracted and uploaded to the
run registry.  From there the values are propagated to the condition database
and the dataset bookkeeping system DBS as described in
section~\ref{certification}.

\subsection{Operation}

In all the offline processing the initial histogram production step is
incorporated in the standard data processing workflows as an additional
execution sequence.  The harvesting step is currently technically implemented
in a number of different ways for different activities.  For Tier-0 the
harvesting and upload to the GUI server are fully integrated into the
processing system.  Currently for other centralised data processing, as the
processing progresses we send semi-manually custom analysis jobs with the CMS
CRAB tool~\cite{crab} to collect the results and upload the returned histogram
file manually to the GUI server.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[!tbp]
\begin{center}
\includegraphics[width=.90\textwidth]{Snippets/Run_registry}
\caption{\label{fig:runregistry}DQM run registry web interface.}
\end{center}
\end{figure}

\section{Certification and sign-off workflow}\label{certification}

CMS uses a {\em run registry} as a central workflow tool to manage the
creation of the final physics dataset certification result.  The run registry
is both a user interface managing the workflow (Fig.~\ref{fig:runregistry}),
and a persistent store of the information; technically speaking it is part of
the online condition database and the web service is hosted as a part of the
WBM system.

The certification process begins with the physicists on online and offline
shift filling in the run registry with basic run information once the run
control system has created the run, and adding any pertitent observations on
the run during the shift.  This information is then augmented with the
automatic data certification results from the online, Tier-0 and Tier-1 data
processing as described in the previous sections.  This results in basic major
detector segment level certification which accounts for DCS, DAQ and DQM
online and offline metrics, and in future also potentially also power and
cooling.  For each detector segment and input one single boolean flag or a
floating point value describes the final quality result.  For the latter we
define for each segment an appropriate threshold such that binary ``good'' or
``bad'' result is obtained.  Were no number was calculated we assign the
result ``unknown.''

Once the automatic certification results are known and uploaded to the run
registry, the person on shift evaluates the detector and physics object
quality following the shift instructions on histograms specifically tailored
to catch relevant problems.  This person adds any final observations to the run
registry and where necessary adds a manual certification result which
overrides the automatic result.

When the final combined quality result has been determined and communicated to
the detector and physics object groups for confirmation, regular sign-off
meetings collect the final verdict and deliver the agreed result to the
experiment.  At this stage the quality flags are copied to the offline
condition database and to the dataset bookkeeping system (DBS)~\cite{dbs}.  In
the latter the quality flags are used to define convenience {\em analysis
  datasets}.  The flags are also available for browsing and inspection via the
CMS data discovery interface~\cite{dbs_discovery}, a web interface to DBS.
The quality flag values stored in the condition database are currently being
made available as normal interval-of-validity based condition data for further
use, e.g. in longer-term data quality evaluation and correlation with other
entities such as temperature information.

For some time trivial trend plots of the key monitor element metrics have been
generated automatically.  We plan to extend this functionality to more
comprehensive trend plotting of any selected histogram metric, and are working
on common design for convenient access and presentation of trends over time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Organisation and operation}

On online the shifts take place mainly at the CMS Point-5 in Cessy, with
remote assistance from the remote CMS centres~\cite{cms_centres_09}.  Offline
shifts are currently operated from the CMS centres at CERN Meyrin site,
Fermilab and DESY.  Standard shift instructions, as illustrated in
Fig.~\ref{fig:shiftdoc}, have been fully exercised.  Perpetual effort to
optimise histograms to maximise sensitivity to problems, to standardise the
look and feel and to improve efficiency through better documentation.

\begin{figure}[!tbp]
\begin{center}
\includegraphics[width=.75\textwidth]{Snippets/DQM_shift_instructions}
\caption{\label{fig:shiftdoc}Example shift instructions.}
\end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experience}

CMS has commissioned a full end to end data quality monitoring system in
tandem with the detector over the last two years.  The online DQM system has
now been in production for about a year and the full offline chain has just
been commissioned.  We have just recently completed the first full cycle of
certification and sign-offs.  DQM for the less structured alignment and
calibration at CAF exists but a fair amount of work remains.
 
In our experience it takes approximately one year to commission a major
component such as online DQM to production quality.  Shift organisation,
instructions, tutorials and supervision are major undertakings in their own
right in particular.  Significant amounts of effort are needed in various
groups to develop the DQM algorithms, and centrally to standardise and
integrate the workflows, procedures, code, systems and servers.  While we find
only modest amounts of code are needed for the DQM core systems, on the
balance there is a perpetual effort to optimise histograms to maximise
sensitivity to problems, to standardise the look and feel and to improve
efficiency through better documentation, and against natural divergence in a
collaboration as large as CMS toward sharing and using common code.

CMS has so far focused on commissioning a common first order DQM system
through-out the entire experiment, with the aim of having an effective basic
system ready for the first beam.  We believe we have achieved this goal and
will address second order features in due course.

CMS is very pleased with the DQM visualisation served using web technology and
operating shifts from the CMS centres.  Remote access to all the DQM
information, especially offsite real-time live access to the online as been
greatly beneficial and appreciated.  Together the CMS centres and remote
access have been essential and practical enabling factors to the experiment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ack

The authors thank the numerous members of CMS collaboration who have
contributed the development and operation of the DQM system.  Effective data
quality monitoring is a truly collaborative effort involving a lot of people
from several other projects: the trigger, detector subsystems, offline and
physics software, production tools, operators, and so on.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{References}
\begin{thebibliography}{9}

\bibitem{cms_tp}
  CMS Collaboration,
  1994, {\it CERN/LHCC 94-38},
  ``Technical proposal''
  (Geneva, Switzerland)

\bibitem{dqm_gui_09}
  Tuura L, Eulisse G, Meyer A,
  2009, {\it Proc. CHEP09, Computing in High Energy Physics},
  ``CMS data quality monitoring web service''
  (Prague, Czech Republic)

\bibitem{caf}
  FIXME: CAF.

\bibitem{root}
  FIXME: ROOT.

\bibitem{runcontrol}
  FIXME: Run control.

\bibitem{wbm}
  FIXME: WBM.

\bibitem{crab}
  FIXME: CRAB.

\bibitem{dbs}
  FIXME: DBS.

\bibitem{dbs_discovery}
  FIXME: DBS discovery.

\bibitem{cms_centres_09}
  FIXME: CMS centres.

\end{thebibliography}
\end{document}
